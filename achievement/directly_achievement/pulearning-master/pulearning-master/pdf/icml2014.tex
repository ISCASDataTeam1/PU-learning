%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2014 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2014,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

% use Times
\usepackage{times}
% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% for big figures
\usepackage{multicol}

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2014} with
% \usepackage[nohyperref]{icml2014} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2014} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Proceedings of the...''
%\usepackage[accepted]{icml2014}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Modified Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

\begin{document} 

\twocolumn[
\icmltitle{Modified Logistic Regression for Training on Incomplete Positive and Unlabeled Datasets}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2014
% package.
\icmlauthor{Joseph Perla}{jjperla@cs.ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}
\icmlauthor{Charles Elkan}{elkan@ucsd.edu}
\icmladdress{University of California, San Diego,
            9500 Gilman Dr, La Jolla, California 92093 USA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{logistic regression, multinomial logistic regression, machine learning, ICML}

\vskip 0.3in
]

\begin{abstract} 
In a common supervised learning task, we are given a training set where all samples are labeled either positive or negative. Semi-supervised algorithms operate on input training sets with some positive and negative labels, and some unlabeled data.  We study the positive-only problem where the training set has an incomplete set of positive labels, no negative labels, and the remainder unlabeled.  In this paper, we assume that the labeled positive samples are selected uniformly at random from the set of all positive samples in the training set with constant probability $c$. Therefore, the labeled and unlabeled positive samples in the training set have identical distributions.  We develop two algorithms ``ceiling logistic regression" and ``positive-only logistic regression". They classify the true labels of the training set given only the restricted set of labels.  In particular, on synthetic data, positive-only logistic regression, trained on a small subset of positive labels performs as well as standard logistic regression, trained on all of the true labels on test data.  We show that we can make strong predictions with very limited positive-only data.

\end{abstract} 

\section{Introduction}

Many datasets do not have full training data that includes a large number of positive and negative labeled examples.  With the increasing size of datasets, often only a small amount of data is manually labeled, while unlabeled data is abundant. Moreover, it is common for one class to be more expensive or even impossible to label.  Example problems in the literature include wildlife habitats \cite{ward08}, and molecular biology databases \cite{elkan08}.  Note that this situation is different from most semi-supervised learning because previous work on semi-supervised learning does have some negatively labeled data.  A synonym for \emph{positive-only} data is \emph{presence-only} data \cite{ward08}.

\subsection{The Problem}

\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{theproblem}}
\caption{An illustration of the two datasets and the two proportions $c$ and $p(y=1)$ inherent in estimating the label probabilities of a positive-only dataaset. }
\label{changingodds}
\end{center}
\vskip -0.2in
\end{figure}

The only input are $N$ feature vectors with a subset of them having the label $\mathbf{1}$ and the rest unlabeled.  

We do not assume that we know the proportion of positive data in the training set ($p(y=1)$) nor do we assume or take as input the probability of labeling a positive instance $c$.  We learn both jointly with all of the models presented in this paper.  We do observe that, in expectation, the proportion of labeled examples ($p(\ell=1)$) in the training set is near the minimum possible $c$ and the minimum possible $p(y=1)$. If $c$ or $p(y=1)$ were much lower than $p(\ell=1)$, then the likelihood of the data would be very low.  We use this fact in positive-only logistic regression.

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{syntheticlr.png}}
\caption{Blue points are positive examples, red negative. The largest dark blue ellipse is the result of training standard LR on all of the data labeled. The smaller light blue ellpise is the result of training standard LR on the positive vs unlabeled examples. The red ellipse is from training Ceiling LR on the positive vs unlabeled examples. The features were $x$, $y$, $x \cdot x$, and $y \cdot y$.  Ceiling LR captures a more representative sample of the positive datapoints, as well as calculating the approximate value of $c = 0.2 \approx 0.196$.   500 positive datapoints were generated using a random gaussian with mean (2, 2) and covariance [[1, 1], [1, 4]].  1000 negative datapoints were generated with mean (-2, -3) and covariance [[4, -1], [-1, 4]]}
\label{synthetic}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Algorithms}

\subsection{Optimal}

Learning on a training set with only some positively labeled data cannot achieve a higher accuracy on the test data than training on the same training dataset where all labels are known (if so, then one could throw out the negative labels in the latter case and achieve the same results).  Therefore, we compare our results to training an SGD logistic regression classifier (LR) and a standard RBF-kernel SVM (SVM) on the training set with all labels known and correctly labeled.  This SVM and all SVMs in this paper were trained using a linear kernel and three-fold cross-validation was used to determine which value of the regularizer $C \in \{1e-5, ..., 1e5\}$ performs optimally on the training set.

\subsection{Baseline}

As a baseline, we show results for running standard logistic regression and an SVM using the positive labeled data versus the unlabeled data.  This overfits to the subset of labeled positive examples which misclassifies many positive examples on the test set.

\subsection{Best Published Algorithms}

\subsubsection{Biased SVM}

State of the art in the literature \cite{elkan08} achieves maximal classification accuracy with a Biased SVM, an SVM where the positive labeled examples were given a different misclassification penalty than the unlabeled examples.  The standard SVM algorithm multiplies all misclassification penalties by the same constant factor $C$ for all examples. A Biased SVM has two values $C_u$ and $C_p$ which multiply the penalty by different factors for errors on unlabeled and positive examples, respectively.

\subsubsection{Weighted SVM}

We also show results for a weighted SVM algorithm, where the SVM classifier is run once to assign probabilities of assignment (after Platt scaling the SVM scores), then those probabilities are used to weigh the inputs to a second SVM classifier which produces the final output.

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{rocproteindata.png}}
\caption{The ROC curves for various learning algorithms on sets of the protein dataset.  Note that we zoom into the most interesting region in the top left.  As expected, both LR and SVM trained on the true labels perform the best and approximately equally on the test set (10 holdout). Standard LR and SVM on the positive data versus the unlabeled data perform poorly.  The best previous work, Biased SVM, does not perform much better than the baselines.  The \emph{ceiling logisting regression} trained on the positive labeled data and unlabeled data performs nearly as well on the test set as the classifiers trained with full knowledge of all positive and negative labels.}
\label{roc}
\end{center}
\vskip -0.2in
\end{figure*}
\subsection{Ceiling Logistic Regression}

One contribution of this paper is a modified version of logistic regression that we solve using stochastic gradient training called \emph{ceiling logistic regression} (CLR).  Whereas standard logistic regression calculates

$$ p(y=1|x) = \frac{1}{1 + e^{w \cdot x}} $$

Our modified logistic regression adds a term $b^2$ which allows a proportion of the unlabeled examples to be classified as having positive probability, but with a ceiling less than 1. From the final value of $b^2$, we calculate the proportion of positive examples which are labeled (taking the assumption that the labeled positive examples are selected uniformly at random and have no bias in which ones are labeled).

$$ p(y=1|x) = \frac{1}{1 + b^2 + e^{w \cdot x}} $$

This algorithm takes into account the unlabeled positive data, performs nearly as well as the classifiers run using the fully labeled training set, and much better than previous work and the baseline naive classifiers.

\subsubsection{Estimating the proportion of labeled examples}

Let $\ell$ be a binary indicator variable signifying if an example is labeled, and let $y$ be the label of the example with features $x$. Let $c = p(\ell=1|y=1)$ the proportion of truly positive examples which are visibly labeled as such. $P$ is the positively labeled data, and $U$ is the unlabeled data. From $c$, we can easily derive $p(y=1)$ the fraction of the data which are truly positive via Bayes's Rule:

\begin{eqnarray*}
p(y=1) &=& \frac{p(s=1)}{p(\ell = 1|y = 1)} \\
 &=&  \frac{|P|}{p(s = 1|y = 1)|P+U|} \\
 &=& \frac{|P|}{c|P+U|}
\end{eqnarray*}

Elkan and Noto \yrcite{elkan08} contains a math error.  Significantly, it concludes that $g(x) = p(\ell=1|x) = c = p(\ell=1|y=1)$. This is incorrect, because it assumes that if a given $x \in P$, then $p(y=1|x) = 1$, however a given set of features may probabilistically be positive for some examples and negative for others. This occurs often when the positive and negative distributions overlap (and never when they are separable). The correct derivation, because $p(\ell=1|y=0)=0$, is

\begin{eqnarray*}
g(x) &=& p(\ell=1|x) \\
 &=& p(\ell=1|x,y=1)p(y=1|x) + 0 \cdot p(y=0|x) \\
 &=& p(\ell=1|x,y=1)p(y=1|x) \\
 &=& c \cdot p(y=1|x)
\end{eqnarray*}

The best estimator of $c$ using this classifier is one based on the trained value of $b$.

$$c \approx \frac{1}{1 + b^2}$$

This is because CLR generates calibrated probabilities in the same way that standard logistic regression does, but instead of having a maximum probability of $1$ for a positive label, it can only have a maximum probability (ceiling) of $p(\ell=1|y=1)=c$. Any positive example has at most probability $c$ of being labeled.  CLR calibrates its probabilities directly and finds $b$ during training of

$$g(x) = \frac{1}{1 + b^2 + e^{w \cdot x}}$$

which has a maximum when $w \cdot x=-\infty$

$$g(x) = \frac{1}{1 + b^2} \approx c$$

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{rocswappedproteindata}}
\caption{We re-run the algorithms from Figure 1 on the same dataset, but swapping the unlabeled positive examples for the labeled positive examples. Our new dataset contains 4558 negative examples, 2453 unlabeled positive examples, and only 348 labeled positive examples.  POLR and Ceiling LR still performs significantly better than baselines and previous work, although not as well as knowing the full label set.  These results are impressive because less than 5\% of the examples are labeled, all of which are positive labels.}
\label{rocswapped}
\end{center}
\vskip -0.2in
\end{figure*}

\begin{figure*}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=120mm]{simulated.png}}
\caption{Simulating two gaussians as above, we can compare logistic regression on the true labels as compared to logistic regression, and POLR on a positive-only dataset with various values of $c$. In this chart, we plot the accuracy on a separate test set generated with the same parameters but varying $c$ from 1\% to 100\%.  \emph{positive-only logistic regression} performs as well as knowing the full training label set even when losing 90+\% of the training labels. Baseline methods require a majority of the label data.}
\label{synthetic}
\end{center}
\vskip -0.2in
\end{figure*}

\section{Positive-Only Logistic Regression}

\emph{Positive-only logistic regression} (POLR) is based on multinomial logistic regression, a multi-class generalization of binary logistic regression also known as softmax regression \cite{renie05}.  In standard 3-class multinomial logistic regression,

$$p(k=1 | x, w_1, w_2, w_3) =  \frac{e^{w_1 \cdot x}}{Z_N}$$
$$p(k=2 | x, w_1, w_2, w_3) =  \frac{e^{w_2 \cdot x}}{Z_N}$$
$$p(k=3 | x, w_1, w_2, w_3) =  \frac{e^{w_3 \cdot x}}{Z_N}$$

$Z_N$ is the normalization factor, the sum of the 3 numerators, and $\sum_{k=1,2,3}{p(k_j=k|x_j, w_1, w_2, w_3)} = 1$.

POLR models the positive-only dataset directly with a 3-class multinomial regression with the following classes: one class is positive and labeled ($P_L$), one is positive and unlabeled ($P_U$), and the last is negative and unlabeled (N).  There are no negative labeled samples.  We use the standard multinomial regression probabilities, but we share the weights $w_p$ between the positive-labeled ($P_L$) and positive-unlabeled ($P_U$) classes. Finally, we include a bias parameter $b$ and a constant $q$ which bias the constant fraction of the positive samples which are labeled to be between $minimumC$ and 1. $c \in [minimumC, 1]$.  We approximate the constant $minimumC$ empirically as the fraction of the training set which are labeled examples.  $Z_N$ is the normalization factor, the sum of the 3 numerators.

$$p(k=P_L | x, w_p, w_n) =  \frac{e^{\mathbf{w_p} \cdot x - \mathbf{b}} + \mathbf{q \cdot e^{w_p \cdot x}}}{Z_N}$$
$$p(k=P_U | x, w_p, w_n) =  \frac{e^{\mathbf{w_p} \cdot x}}{Z_N}$$
$$p(k=N | x, w_p, w_n) =  \frac{e^{w_n \cdot x}}{Z_N}$$

This simplifies (proof in the Supplementary Materials) to

$$ p(k=P_L | x, w) = \frac{e^{-b} + q}{1 + e^{-b} + e^{w \cdot x}}$$
$$ p(k=P_U | x, w) =  \frac{1}{1 + e^{-b} + e^{w \cdot x}}$$
$$ p(k=N | x, w) = \frac{e^{w \cdot x}}{1 + e^{-b} + e^{w \cdot x}}$$

Where $w = w_n - w_p$. Notice that, $\forall x$,

$$\frac{p(k=P_L | x, w)}{p(k=P_L | x, w) + p(k=P_U | x, w)} = \frac{1}{1 + q + e^{-b}} = c$$

Thus, $c$ is the constant fraction of positive samples which are labeled. The term $q$ ensures that $c$ stays between $minimumC$ and $1$, instead of  between $0$ and $1$.  The constant $q$, affecting the lower bound of $c$, does not change during or after training. The upper bound of $c$ is 1.

$$q = \frac{1}{1 - minimumC} - 1$$

\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{changingodds.png}}
\caption{The $w$ parameter update changes the odds of the positive and negatives overall, but does not change $c$ the probability of being labeled. Updates on this parameter change the likelihood of the observed labels while also changing the overall probability of positive labels in the same direction. More positive or negative data will push it in the appropriate direction. $n_j = p(k=N|x_j,w)$ and $p_j = 1-n_j$. The likelihood is maximized when $c \cdot p_{j}$ is maximal for labeled points and minimized for unlabeled points. }
\label{changingodds}
\end{center}
\vskip -0.2in
\end{figure}

\begin{figure}[ht!]
\vskip 0.2in
\begin{center}
\centerline{\includegraphics[width=\columnwidth]{changingc.png}}
\caption{The $b$ parameter update changes $c$ the probability of being labeled, but does not affect the proportion of positive to negative at each example. Updates to $b$ increase the likelihood of the observed data only without affecting the proportion of positive vs negative data overall. It is a weighted average of the observed label proportions.  It reaches a stable point when $\sum_{j}{p(\ell_j=1|x_j,w)}$ is the true proportion of labeled data.}
\label{changingc}
\end{center}
\vskip -0.2in
\end{figure}

\subsection{Learning $\beta = b, w$ from $D$}

Assuming we have independently drawn data $D$, the conditional log likelihood, where $\beta = b, w$ is \cite{carpenter08}

\begin{eqnarray*}
\log{p(D | \beta)} &=& \log \prod_{j=1...n}{p(\ell_j, x_j | \beta)} \\
 &=& \sum_{j=1...n}{\log{p(\ell_j, x_j | \beta)}} \\
 &=& \sum_{j=1...n}{\log{p(\ell_j | x_j, \beta)}} \cdot p(x_j | \beta)
\end{eqnarray*}

where $\ell_j$ is an indicator variable describing whether the sample is labeled or not. $\ell_j = 1$ if and only if $k_j=P_L$, otherwise $k_j = N$ or $k_j=P_U$ and $\ell_j = 0$.  Also note that $p(x_j|\beta) = p(x_j)$ and we assume an uninformative uniform prior for $x_j$, so let $p(x_j|\beta) = 1$.

The maximum likelihood (ML) estimate $\hat \beta$ is the value of $\beta$ maximizing the likelihood of the data $D$:
$$
\hat \beta = arg max_{\beta} p(D | \beta) = arg max_{\beta} \log{p(D | \beta)}.
$$

We calculate the gradients (derivations in the Supplementary Materials) in order to perform stochastic gradient ascent to maximize the log likelihood.

\begin{eqnarray*}
\nabla_{w_i}{Err_R (x_j, \beta)} = x_{j,i} \cdot
    \big(	I(\ell_j=0) \cdot p(k_j=N | \ell_j=0) \\
		- p(k_j=N)
    \big)
\end{eqnarray*}

\begin{eqnarray*}
\nabla_{b}{Err_R (x_j, \beta)}  = \Big( \frac{q}{Z} - p(k=P_L | x_j,\beta) \Big) \cdot \\
	\left( 
		\frac{I(\ell_j=1)}{p(k=P_L | x_j,\beta)} + 1 
	  \right)
\end{eqnarray*}

As in Ceiling LR, we do not regularize the weights in our implementation.

\subsection{Convexity}

POLR's error function is convex for the same reason that standard Logistic Regression is convex. The error function is the sum of the log of a number of individual likelihoods for each training sample. Each of these is an affine function term plus a log of the sum of an exponential of an affine function. The log of the sum of an exponential is convex, as is the affine function.  Therefore, all of them summed together are also convex.  If we were to add regularization, then it would be strictly convex.  Strict convexity is less important for stochastic gradient training we do here because in SGD the gradient is not computed exactly anyway and therefore will likely always point somewhere.

\section{Experiments}

\subsection{Implementation}

Each algorithm was implemented in less than 300 lines of Python and Cython. Training is fast: each epoch takes under 1 second to train using SGD.

\subsection{Synthetic Data}

Figure \ref{synthetic} reproduce a version of gaussians similar to those generated in \cite{elkan08}. 

\subsection{SwissProt Dataset}

The algorithms were run on the SwissProt \cite{elkan08} dataset of 7359 proteins which each have a sparse set of 24081 binary features related to characteristics of the proteins. It contains 4558 negative examples, 2453 labeled positive examples, and 348 unlabeled positive examples.   We consider the negative and unlabeled positive examples together as the unlabeled dataset. Results are illustrated in Figure \ref{roc}. To make the problem more difficult, we swapped the $P_L$ and $P_U$ classes and ran all of the algorithms again with results in Figure \ref{rocswapped}.

\section{Discussion}

The power of POLR comes from the assumption that the labeling is done uniformly at random on the positive samples.  The stochastic gradient training then optimizes $w$ so that the probability of the labeled examples is high, while simultaneously having $c$ slack to mislabel by jointly training the bias parameter $b$.  This is similar to the regularization parameter $C$ in SVMs, however it has a clearer interpretation.  Moreover, POLR does not require a non-probabilistic regularization parameter added to its likelihood function to achieve state-of-the-art results.

\section{Related Work}

The positive-only case can be seen as a special case of label-noise with 0 probability of labeling negative data. Bootkrajang and Kaban \yrcite{bootkrajang12} designed a robust logistic regression algorithm based on binary logistic regression that has an extra variable indicating the true labels.  Unlike POLR, robust logistic regression is not convex and requires multiple restarts with different initializations and does not guarantee an optimal result. Tibshirani and Manning \yrcite{tibshirani13} created a robust logistic regression algorithm with  shift parameters in the exponential term.  It is a convex model but it adds $N$ parameters to the model where $N$ are the number of samples.  This makes the model much more complicated and is problematic if $N >> P$.

\section{Future Work}

The performance on real-world SwissProt data was poorer than on synthetic data likely because the probability of choosing $c$ is not uniform but biased in some way. Ideally, we would like to be able to relax the assumption that $p(\ell_j|x)=c$ for all $x$.  

It may be possible to extend POLR to the semi-supervised setting, where we also have some negative labels, in order to solve the label-noise problem.

It would also be interesting to extend this to the multi-class or multi-label setting, especially since it is already based on multinomial logistic regression. Finally, the true label ends up being a hidden variable in the model. It would be interesting to see if we can model more complex hidden variable interactions with logistic regression.

\section{Conclusions}

We presented several algorithms for learning from positive only data. Large amounts of unlabeled data are common, and it can be expensive to label more than a tiny amount of it.  Moreover, one class may be more expensive to label than another. Synthetic experiments show that POLR on just 1\% of the just positive labels can perform as well as standard LR on the true labels, saving orders of magnitude in labeling costs.

\bibliography{icml2014}
\bibliographystyle{icml2014}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  
